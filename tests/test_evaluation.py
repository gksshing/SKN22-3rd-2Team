"""
ì‡¼íŠ¹í—ˆ (Short-Cut) v3.0 - DeepEval RAG Quality Tests
=====================================================
RAG íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ ê²€ì¦ í…ŒìŠ¤íŠ¸ (Faithfulness, Answer Relevancy)

Metrics:
1. FaithfulnessMetric - ë‹µë³€ì´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ëŠ”ì§€ ê²€ì¦
2. AnswerRelevancyMetric - ë‹µë³€ì´ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ”ì§€ ê²€ì¦

Team: ë€¨ğŸ’•
"""

import pytest
import asyncio
import sys
import os
from pathlib import Path
from typing import List, Dict, Any

# Add src to path
# Add project root to path (so 'src' package is resolvable)
sys.path.insert(0, str(Path(__file__).parent.parent))

# Load Env
from dotenv import load_dotenv
load_dotenv()

# Check for required environment variables
if not os.environ.get("OPENAI_API_KEY"):
    pytest.skip(
        "OPENAI_API_KEY not set. Skipping DeepEval tests.",
        allow_module_level=True
    )

# DeepEval imports
try:
    from deepeval import assert_test
    from deepeval.test_case import LLMTestCase
    from deepeval.metrics import FaithfulnessMetric, AnswerRelevancyMetric
except ImportError:
    pytest.skip(
        "deepeval not installed. Run: pip install deepeval",
        allow_module_level=True
    )

# Import PatentAgent
from src.patent_agent import PatentAgent


# =============================================================================
# Golden Dataset - AI/NLP Domain Test Cases
# =============================================================================
# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ëŠ” ìš°ë¦¬ ë°ì´í„°ì˜ ë„ë©”ì¸ í‚¤ì›Œë“œì— ë§ê²Œ ì„¤ê³„ë¨:
# - retrieval augmented generation
# - large language model
# - neural information retrieval
# - semantic search
# - document embedding
# - transformer attention
# - knowledge graph reasoning
# - prompt engineering

GOLDEN_DATASET: List[Dict[str, Any]] = [
    {
        "id": "test_001",
        "name": "RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ",
        "query": """
        Please generate a comprehensive patent analysis report for the following idea
        (including prior art search, infringement risk, and avoidance strategy):
        A document search system utilizing Retrieval Augmented Generation technology.
        It converts user queries into vector embeddings, retrieves similar documents 
        from a vector database, and provides them as context to an LLM.
        It uses hybrid search (Dense + Sparse) and RRF fusion.
        """,
        "expected_topics": ["retrieval", "embedding", "vector", "search"],
    },
    {
        "id": "test_002",
        "name": "Semantic Search ì—”ì§„",
        "query": """
        Please generate a comprehensive patent analysis report for the following idea
        (including prior art search, infringement risk, and avoidance strategy):
        A semantic search engine based on Neural information retrieval.
        It embeds documents and queries using Transformer models and 
        retrieves semantically similar documents via cosine similarity.
        It provides more accurate results than traditional keyword search.
        """,
        "expected_topics": ["semantic", "transformer", "embedding", "neural"],
    },
    {
        "id": "test_003",
        "name": "LLM Fine-tuning ì‹œìŠ¤í…œ",
        "query": """
        Please generate a comprehensive patent analysis report for the following idea
        (including prior art search, infringement risk, and avoidance strategy):
        A system for fine-tuning Large Language Models on specific domains.
        It applies quantization techniques for efficient inference and 
        generates optimized results via prompt engineering.
        """,
        "expected_topics": ["language model", "fine-tuning", "inference", "prompt"],
    },
]


# =============================================================================
# Test Configuration
# =============================================================================

# Metric thresholds
FAITHFULNESS_THRESHOLD = 0.7
RELEVANCY_THRESHOLD = 0.7

# Evaluation model
EVAL_MODEL = "gpt-4o-mini"  # Cost-effective for testing


# =============================================================================
# Fixtures
# =============================================================================

@pytest.fixture(scope="module")
def event_loop():
    """Create event loop for async tests."""
    loop = asyncio.new_event_loop()
    yield loop
    loop.close()


@pytest.fixture(scope="module")
def patent_agent():
    """Initialize PatentAgent (expensive, so module-scoped)."""
    try:
        agent = PatentAgent()
        return agent
    except Exception as e:
        pytest.skip(f"Failed to initialize PatentAgent: {e}")


# =============================================================================
# Helper Functions
# =============================================================================

def extract_retrieval_context(search_results: List[Dict]) -> List[str]:
    """
    Extract retrieval context from search results.
    
    Combines abstract and claims from each patent for full context.
    This is critical for DeepEval FaithfulnessMetric accuracy.
    """
    context = []
    for result in search_results:
        patent_id = result.get("patent_id", "Unknown")
        title = result.get("title", "")
        abstract = result.get("abstract", "")
        claims = result.get("claims", "")
        
        # Build comprehensive context string
        context_parts = [f"Patent {patent_id}: {title}"]
        
        if abstract:
            # Truncate abstract if too long (DeepEval has token limits, but 4o-mini handles 128k)
            # Increased from 1500 to 4000 chars to cover full context
            abstract_truncated = abstract[:4000] if len(abstract) > 4000 else abstract
            context_parts.append(f"Abstract: {abstract_truncated}")
        
        if claims:
            # Truncate claims if too long
            # Increased from 2000 to 5000 chars to ensure critical claims are included
            claims_truncated = claims[:5000] if len(claims) > 5000 else claims
            context_parts.append(f"Claims: {claims_truncated}")
        
        if result.get("grading_reason"):
            context_parts.append(f"Relevance: {result['grading_reason']}")
        
        context.append(" | ".join(context_parts))
    
    return context


def extract_actual_output(analysis: Dict) -> str:
    """
    Extract actual output string from analysis result.
    
    Combines key analysis sections into a single string.
    """
    parts = []
    
    # Similarity
    sim = analysis.get("similarity", {})
    if sim.get("summary"):
        parts.append(f"ìœ ì‚¬ë„ ë¶„ì„: {sim['summary']} (ì ìˆ˜: {sim.get('score', 'N/A')})")
    
    # Infringement
    inf = analysis.get("infringement", {})
    if inf.get("summary"):
        parts.append(f"ì¹¨í•´ ë¦¬ìŠ¤í¬: {inf['summary']} (ë ˆë²¨: {inf.get('risk_level', 'N/A')})")
    
    # Avoidance
    avoid = analysis.get("avoidance", {})
    if avoid.get("summary"):
        parts.append(f"íšŒí”¼ ì „ëµ: {avoid['summary']}")
    
    # Conclusion
    if analysis.get("conclusion"):
        parts.append(f"ê²°ë¡ : {analysis['conclusion']}")
    
    return " | ".join(parts) if parts else "No analysis available"


async def run_agent_analysis(agent: PatentAgent, query: str) -> Dict[str, Any]:
    """Run PatentAgent analysis asynchronously."""
    return await agent.analyze(query, use_hybrid=True, stream=False)


# =============================================================================
# DeepEval Metric Instances
# =============================================================================

@pytest.fixture(scope="module")
def faithfulness_metric():
    """Create FaithfulnessMetric instance."""
    return FaithfulnessMetric(
        threshold=FAITHFULNESS_THRESHOLD,
        model=EVAL_MODEL,
        include_reason=True,
    )


@pytest.fixture(scope="module")
def relevancy_metric():
    """Create AnswerRelevancyMetric instance."""
    return AnswerRelevancyMetric(
        threshold=RELEVANCY_THRESHOLD,
        model=EVAL_MODEL,
        include_reason=True,
    )


# =============================================================================
# Test Class: RAG Quality Evaluation
# =============================================================================

class TestRAGQuality:
    """
    RAG íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤.
    
    DeepEvalì˜ FaithfulnessMetricê³¼ AnswerRelevancyMetricì„ ì‚¬ìš©í•˜ì—¬
    PatentAgentì˜ ë¶„ì„ ê²°ê³¼ê°€ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ì¶©ì‹¤í•œì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    
    @pytest.mark.asyncio
    @pytest.mark.integration
    @pytest.mark.slow
    @pytest.mark.parametrize("test_case", GOLDEN_DATASET, ids=lambda tc: tc["id"])
    async def test_rag_quality(
        self,
        test_case: Dict[str, Any],
        patent_agent: PatentAgent,
        faithfulness_metric: FaithfulnessMetric,
        relevancy_metric: AnswerRelevancyMetric,
        record_property,
    ):
        """
        RAG í’ˆì§ˆ í…ŒìŠ¤íŠ¸: Faithfulness + Answer Relevancy.
        
        Args:
            test_case: Golden datasetì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
            patent_agent: PatentAgent ì¸ìŠ¤í„´ìŠ¤
            faithfulness_metric: ì¶©ì‹¤ë„ ë©”íŠ¸ë¦­
            relevancy_metric: ê´€ë ¨ì„± ë©”íŠ¸ë¦­
            record_property: pytest fixture for custom report attributes
        """
        print(f"\n{'='*60}")
        print(f"ğŸ§ª Test Case: {test_case['name']}")
        print(f"{'='*60}")
        
        query = test_case["query"].strip()
        
        # Step 1: Run PatentAgent analysis
        try:
            result = await run_agent_analysis(patent_agent, query)
        except Exception as e:
            pytest.fail(f"PatentAgent.analyze() failed: {e}")
        
        # Check for errors
        if "error" in result:
            pytest.skip(f"No patents found: {result['error']}")
        
        # Step 2: Extract components for LLMTestCase
        search_results = result.get("search_results", [])
        analysis = result.get("analysis", {})
        
        # Input: User's idea
        input_text = query
        
        # Actual Output: Analysis conclusion/summary
        actual_output = extract_actual_output(analysis)
        
        # Retrieval Context: Patent abstracts/claims
        retrieval_context = extract_retrieval_context(search_results)
        
        print(f"\nğŸ“¥ Input Query: {input_text[:100]}...")
        print(f"ğŸ“¤ Actual Output: {actual_output[:200]}...")
        print(f"ğŸ“š Context Count: {len(retrieval_context)}")
        
        # Step 3: Create LLMTestCase
        llm_test_case = LLMTestCase(
            input=input_text,
            actual_output=actual_output,
            retrieval_context=retrieval_context,
        )
        
        # Step 4: Measure and assert with DeepEval metrics
        print("\nğŸ” Running DeepEval metrics...")
        
        # Measure Faithfulness (this computes the score)
        faithfulness_metric.measure(llm_test_case)
        faith_score = faithfulness_metric.score
        faith_reason = faithfulness_metric.reason
        
        # Record to XML report
        record_property("faithfulness_score", faith_score)
        record_property("faithfulness_reason", faith_reason or "N/A")
        
        print(f"   ğŸ“Š Faithfulness Score: {faith_score:.2f} (threshold: {FAITHFULNESS_THRESHOLD})")
        if faith_reason:
            print(f"      Reason: {faith_reason[:150]}...")
        
        # Assert Faithfulness
        if faith_score < FAITHFULNESS_THRESHOLD:
            raise AssertionError(f"Faithfulness score {faith_score:.2f} < {FAITHFULNESS_THRESHOLD}")
        print(f"   âœ… Faithfulness: PASSED")
        
        # Measure Answer Relevancy
        relevancy_metric.measure(llm_test_case)
        rel_score = relevancy_metric.score
        rel_reason = relevancy_metric.reason
        
        # Record to XML report
        record_property("relevancy_score", rel_score)
        record_property("relevancy_reason", rel_reason or "N/A")

        print(f"   ğŸ“Š Answer Relevancy Score: {rel_score:.2f} (threshold: {RELEVANCY_THRESHOLD})")
        if rel_reason:
            print(f"      Reason: {rel_reason[:150]}...")
        
        # Assert Relevancy
        if rel_score < RELEVANCY_THRESHOLD:
            raise AssertionError(f"Answer Relevancy score {rel_score:.2f} < {RELEVANCY_THRESHOLD}")
        print(f"   âœ… Answer Relevancy: PASSED")
        
        print(f"\n{'='*60}")
        print(f"âœ… Test Case '{test_case['name']}' PASSED")
        print(f"{'='*60}")
    



# =============================================================================
# Standalone Test Functions (Alternative to Class)
# =============================================================================

@pytest.mark.asyncio
@pytest.mark.integration
async def test_single_query_quality(patent_agent: PatentAgent):
    """
    ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ ë¹ ë¥¸ í’ˆì§ˆ ê²€ì¦ í…ŒìŠ¤íŠ¸.
    
    CI/CD íŒŒì´í”„ë¼ì¸ì—ì„œ ë¹ ë¥´ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ê²½ëŸ‰ í…ŒìŠ¤íŠ¸.
    """
    query = "Natural Language Processing based Patent Search System"
    
    result = await patent_agent.analyze(query, use_hybrid=True)
    
    # Basic assertions (not DeepEval)
    assert "analysis" in result, "Analysis should be present in result"
    assert "search_results" in result, "Search results should be present"
    assert result.get("analysis", {}).get("conclusion"), "Conclusion should not be empty"
    
    print(f"âœ… Single query test passed")
    print(f"   - Found {len(result.get('search_results', []))} patents")
    print(f"   - Similarity score: {result['analysis']['similarity'].get('score')}")


# =============================================================================
# Run Tests
# =============================================================================

if __name__ == "__main__":
    pytest.main([
        __file__,
        "-v",
        "--tb=short",
        "-x",  # Stop on first failure
        "--asyncio-mode=auto",
    ])
